# Copyright 2024 The HuggingFace Inc. team. All rights reserved.
# Adapted for evaluating transformer-based noise critic.

"""
This script demonstrates how to evaluate a policy composed of separately trained
diffusion, inverse dynamics, and transformer-based noise critic models.

It evaluates how well the noise critic can distinguish between clean trajectories
generated by the diffusion model and artificially noisy versions of those trajectories.
"""

import torch
import numpy as np
from pathlib import Path
import gym_pusht  # noqa: F401
import gymnasium as gym
import imageio
import json
from typing import Dict, List, Any

# Import necessary components
from model.diffusion.configuration_mymodel import DiffusionConfig
from model.diffusion.modeling_mymodel import MyDiffusionModel
from model.diffusion.modeling_combined import CombinedPolicy
from model.invdynamics.invdyn import MlpInvDynamic
from model.critic.noise_critic import create_noise_critic, NoiseCriticConfig
from lerobot.common.datasets.lerobot_dataset import LeRobotDatasetMetadata
from sklearn.metrics import roc_auc_score, accuracy_score


def main():
    # --- Configuration ---
    # Define paths to the individual component outputs and config/stats
    diffusion_output_dir = Path("outputs/train/diffusion_only")
    invdyn_output_dir = Path("outputs/train/invdyn_only")
    noise_critic_output_dir = Path("outputs/train/noise_critic")

    # Config path for diffusion model
    config_stats_path = diffusion_output_dir

    # Output directory for evaluation results
    output_directory = Path("outputs/eval/noise_critic")
    output_directory.mkdir(parents=True, exist_ok=True)

    # Device setup
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Noise levels to evaluate
    noise_levels = [0.05, 0.1, 0.2]

    # Number of episodes
    num_episodes = 5

    # Whether to save video
    save_video = True

    # --- Load Config and Stats ---
    cfg_path = Path(config_stats_path) / "config.json"
    data = json.loads(cfg_path.read_text())
    cfg = DiffusionConfig(**data)
    cfg.device = device  # Override device if needed

    # Load dataset metadata and stats
    metadataset_stats = LeRobotDatasetMetadata("lerobot/pusht")
    dataset_stats = {}
    for key, stat in metadataset_stats.stats.items():
        dataset_stats[key] = {
            subkey: torch.as_tensor(subval, dtype=torch.float32, device=device)
            for subkey, subval in stat.items()
        }

    # --- Load Models ---
    # Diffusion Model
    diffusion_ckpt_path = diffusion_output_dir / "diffusion_final.pth"
    if not diffusion_ckpt_path.is_file():
        raise OSError(
            f"Diffusion checkpoint not found at {diffusion_ckpt_path}")
    diffusion_model = MyDiffusionModel(cfg)
    print(f"Loading diffusion state dict from: {diffusion_ckpt_path}")
    diff_state_dict = torch.load(diffusion_ckpt_path, map_location=device)
    diffusion_model.load_state_dict(diff_state_dict)
    diffusion_model.eval()
    diffusion_model.to(device)

    # Inverse Dynamics Model
    invdyn_ckpt_path = invdyn_output_dir / "invdyn_final.pth"
    if not invdyn_ckpt_path.is_file():
        raise OSError(
            f"Inverse dynamics checkpoint not found at {invdyn_ckpt_path}")
    inv_dyn_model = MlpInvDynamic(
        # State dim * 2 (current + prev)
        o_dim=cfg.robot_state_feature.shape[0] * 2,
        a_dim=cfg.action_feature.shape[0],
        hidden_dim=cfg.inv_dyn_hidden_dim,
        dropout=0.1,
        use_layernorm=True,
        out_activation=torch.nn.Tanh()
    )
    print(f"Loading invdyn state dict from: {invdyn_ckpt_path}")
    inv_state_dict = torch.load(invdyn_ckpt_path, map_location=device)
    inv_dyn_model.load_state_dict(inv_state_dict)
    inv_dyn_model.eval()
    inv_dyn_model.to(device)

    # Create combined model
    combined_model = CombinedPolicy(diffusion_model, inv_dyn_model)

    # Noise Critic Model - explicitly using the TransformerCritic architecture
    noise_critic_ckpt_path = noise_critic_output_dir / "noise_critic_final.pth"
    if not noise_critic_ckpt_path.is_file():
        raise OSError(
            f"Noise critic checkpoint not found at {noise_critic_ckpt_path}")

    # Create noise critic config
    critic_cfg = NoiseCriticConfig(
        state_dim=cfg.robot_state_feature.shape[0],
        horizon=cfg.horizon,
        hidden_dim=cfg.hidden_dim,
        num_layers=cfg.num_layers,
        dropout=cfg.dropout,
        use_layernorm=cfg.use_layernorm,
        architecture="transformer",  # Explicitly use transformer architecture
        use_image_context=cfg.use_image_observations
    )

    # Create and load critic model
    print(f"Loading noise critic state dict from: {noise_critic_ckpt_path}")
    noise_critic_model = create_noise_critic(critic_cfg)
    noise_critic_state_dict = torch.load(
        noise_critic_ckpt_path, map_location=device)
    noise_critic_model.load_state_dict(noise_critic_state_dict)
    noise_critic_model.eval()
    noise_critic_model.to(device)
    print("Transformer-based Noise critic model loaded successfully.")

    # --- Environment Setup ---
    env = gym.make(
        "gym_pusht/PushT-v0",
        obs_type="pixels_agent_pos",  # Ensure this matches config expectations
        max_episode_steps=500,
    )

    # --- Evaluation Loop ---
    all_results = []
    for episode in range(num_episodes):
        # Reset environment and model
        numpy_observation, info = env.reset(seed=42 + episode)
        combined_model.reset()

        rewards = []
        frames = []
        frames.append(env.render())

        # Store noise critic scores
        critic_scores = {level: [] for level in noise_levels}
        critic_scores["original"] = []

        step = 0
        done = False

        print(f"\nStarting evaluation episode {episode+1}/{num_episodes}...")

        while not done:
            # --- Prepare Observation ---
            state_np = numpy_observation["agent_pos"]
            image_np = numpy_observation["pixels"]

            state = torch.from_numpy(state_np).to(torch.float32)
            image = torch.from_numpy(image_np).to(torch.float32) / 255
            image = image.permute(2, 0, 1)  # HWC to CHW

            # Add batch dim and move to device
            state = state.unsqueeze(0).to(device)
            image = image.unsqueeze(0).to(device)

            # Create observation dictionary for the model
            observation = {
                "observation.state": state,
                "observation.image": image,
            }

            # Get action and trajectories from the combined policy
            with torch.inference_mode():
                # For the combined policy we need to get trajectories as well
                action, trajectories = combined_model.predict_with_trajectories(
                    curr_state=observation["observation.state"],
                    prev_state=observation["observation.state"] if step == 0 else prev_state,
                    image=image
                )

            # Extract trajectory for noise critic (take first trajectory)
            # Shape: [horizon, state_dim]
            normalized_trajectory = trajectories[0]

            # Score original trajectory with noise critic
            with torch.inference_mode():
                orig_score = noise_critic_model(
                    trajectory_sequence=normalized_trajectory.unsqueeze(0)
                ).squeeze().item()
                critic_scores["original"].append(orig_score)

            # Score noisy trajectories
            for noise_level in noise_levels:
                # Apply noise to trajectory
                noisy_traj = normalized_trajectory.clone(
                ) + torch.randn_like(normalized_trajectory) * noise_level

                # Get critic score
                with torch.inference_mode():
                    noise_score = noise_critic_model(
                        trajectory_sequence=noisy_traj.unsqueeze(0)
                    ).squeeze().item()
                    critic_scores[noise_level].append(noise_score)

            # Convert to numpy for environment step
            numpy_action = action.squeeze(0).cpu().numpy()

            # Store previous state
            prev_state = observation["observation.state"]

            # Step the environment
            numpy_observation, reward, terminated, truncated, info = env.step(
                numpy_action)

            print(
                f"Episode {episode+1}, Step {step}: Reward = {reward:.4f}, Terminated = {terminated}")

            rewards.append(reward)
            frames.append(env.render())

            done = terminated or truncated or done
            step += 1

        if terminated:
            print(f"Episode {episode+1} completed successfully!")
        else:
            print(f"Episode {episode+1} ended without success.")

        # Convert scores to numpy arrays
        for k in critic_scores:
            critic_scores[k] = np.array(critic_scores[k])

        # Calculate episode metrics
        episode_metrics = {
            "episode": episode + 1,
            "return": sum(rewards),
            "steps": step,
            "success": terminated,
            "auc": [],
            "accuracy": []
        }

        # Calculate classifier metrics for each noise level
        for noise_level in noise_levels:
            # Combine original and noisy scores
            all_scores = np.concatenate([
                critic_scores["original"],
                critic_scores[noise_level]
            ])

            # Create labels (1 for original, 0 for noisy)
            all_labels = np.concatenate([
                np.ones_like(critic_scores["original"]),
                np.zeros_like(critic_scores[noise_level])
            ])

            # Apply sigmoid to convert logits to probabilities
            all_probs = 1 / (1 + np.exp(-all_scores))

            # Calculate AUC and accuracy
            auc = roc_auc_score(all_labels, all_probs)
            preds = (all_probs > 0.5).astype(float)
            accuracy = accuracy_score(all_labels, preds)

            episode_metrics["auc"].append(auc)
            episode_metrics["accuracy"].append(accuracy)

            print(
                f"  Noise level {noise_level}: AUC = {auc:.4f}, Accuracy = {accuracy:.4f}")

        all_results.append({
            "metrics": episode_metrics,
            "critic_scores": {k: v.tolist() for k, v in critic_scores.items()}
        })

        # Save video for this episode if requested
        if save_video:
            fps = env.metadata["render_fps"]
            video_path = output_directory / f"episode_{episode+1}.mp4"
            imageio.mimsave(str(video_path), np.stack(frames), fps=fps)
            print(f"  Video saved to {video_path}")

    # --- Aggregate Results ---
    # Calculate overall metrics
    overall_metrics = {
        "num_episodes": num_episodes,
        "noise_levels": noise_levels,
        "returns": [r["metrics"]["return"] for r in all_results],
        "mean_return": np.mean([r["metrics"]["return"] for r in all_results]),
        "std_return": np.std([r["metrics"]["return"] for r in all_results]),
        "success_rate": np.mean([r["metrics"]["success"] for r in all_results]),
        "mean_auc": [np.mean([r["metrics"]["auc"][i] for r in all_results]) for i in range(len(noise_levels))],
        "mean_accuracy": [np.mean([r["metrics"]["accuracy"][i] for r in all_results]) for i in range(len(noise_levels))],
    }

    # Print overall results
    print("\n=== Overall Results ===")
    print(f"Episodes: {num_episodes}")
    print(
        f"Mean Return: {overall_metrics['mean_return']:.4f} ± {overall_metrics['std_return']:.4f}")
    print(f"Success Rate: {overall_metrics['success_rate']:.4f}")
    print("Noise Critic Performance:")
    for i, level in enumerate(noise_levels):
        print(f"  Noise Level {level}: Mean AUC = {overall_metrics['mean_auc'][i]:.4f}, "
              f"Mean Accuracy = {overall_metrics['mean_accuracy'][i]:.4f}")

    # Save results to JSON
    results_file = output_directory / "evaluation_results.json"
    with open(results_file, "w") as f:
        json.dump({
            "overall_metrics": overall_metrics,
            "episode_results": all_results
        }, f, indent=4)
    print(f"Results saved to {results_file}")

    # Close environment
    env.close()
    print("Evaluation complete!")


if __name__ == "__main__":
    main()
