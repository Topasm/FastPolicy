import math
from collections import deque
from typing import Callable

import einops
import numpy as np
import torch
import torch.nn.functional as F  # noqa: N812
import torchvision
from diffusers.schedulers.scheduling_ddim import DDIMScheduler
from diffusers.schedulers.scheduling_ddpm import DDPMScheduler
from torch import Tensor, nn

from lerobot.common.constants import OBS_ENV, OBS_ROBOT
from lerobot.common.policies.diffusion.configuration_diffusion import DiffusionConfig
from lerobot.common.policies.normalize import Normalize, Unnormalize
from lerobot.common.policies.pretrained import PreTrainedPolicy
from lerobot.common.policies.utils import (
    get_device_from_parameters,
    get_dtype_from_parameters,
    get_output_shape,
    populate_queues,
)


class DiffusionPolicy(PreTrainedPolicy):
    """
    Diffusion Policy as per "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion"
    (paper: https://arxiv.org/abs/2303.04137, code: https://github.com/real-stanford/diffusion_policy).
    """

    config_class = DiffusionConfig
    name = "diffusion"

    def __init__(
        self,
        config: DiffusionConfig,
        dataset_stats: dict[str, dict[str, Tensor]] | None = None,
    ):
        """
        Args:
            config: Policy configuration class instance or None, in which case the default instantiation of
                the configuration class is used.
            dataset_stats: Dataset statistics to be used for normalization. If not passed here, it is expected
                that they will be passed with a call to `load_state_dict` before the policy is used.
        """
        super().__init__(config)
        config.validate_features()
        self.config = config

        self.normalize_inputs = Normalize(
            config.input_features, config.normalization_mapping, dataset_stats)
        self.normalize_targets = Normalize(
            config.output_features, config.normalization_mapping, dataset_stats
        )
        self.unnormalize_outputs = Unnormalize(
            config.output_features, config.normalization_mapping, dataset_stats
        )

        # queues are populated during rollout of the policy, they contain the n latest observations and actions
        self._queues = None

        self.diffusion = DiffusionModel(config)

        self.reset()

    def get_optim_params(self) -> dict:
        return self.diffusion.parameters()

    def reset(self):
        """Clear observation and action queues. Should be called on `env.reset()`"""
        self._queues = {
            "observation.state": deque(maxlen=self.config.n_obs_steps),
            "action": deque(maxlen=self.config.n_action_steps),
        }
        if self.config.image_features:
            self._queues["observation.images"] = deque(
                maxlen=self.config.n_obs_steps)
        if self.config.env_state_feature:
            self._queues["observation.environment_state"] = deque(
                maxlen=self.config.n_obs_steps)

    @torch.no_grad
    def select_action(self, batch: dict[str, Tensor]) -> Tensor:
        """Select a single action given environment observations.

        This method handles caching a history of observations and an action trajectory generated by the
        underlying diffusion model. Here's how it works:
          - `n_obs_steps` steps worth of observations are cached (for the first steps, the observation is
            copied `n_obs_steps` times to fill the cache).
          - The diffusion model generates `horizon` steps worth of actions.
          - `n_action_steps` worth of actions are actually kept for execution, starting from the current step.
        Schematically this looks like:
            ----------------------------------------------------------------------------------------------
            (legend: o = n_obs_steps, h = horizon, a = n_action_steps)
            |timestep            | n-o+1 | n-o+2 | ..... | n     | ..... | n+a-1 | n+a   | ..... | n-o+h |
            |observation is used | YES   | YES   | YES   | YES   | NO    | NO    | NO    | NO    | NO    |
            |action is generated | YES   | YES   | YES   | YES   | YES   | YES   | YES   | YES   | YES   |
            |action is used      | NO    | NO    | NO    | YES   | YES   | YES   | NO    | NO    | NO    |
            ----------------------------------------------------------------------------------------------
        Note that this means we require: `n_action_steps <= horizon - n_obs_steps + 1`. Also, note that
        "horizon" may not the best name to describe what the variable actually means, because this period is
        actually measured from the first observation which (if `n_obs_steps` > 1) happened in the past.
        """
        batch = self.normalize_inputs(batch)
        if self.config.image_features:
            # shallow copy so that adding a key doesn't modify the original
            batch = dict(batch)
            batch["observation.images"] = torch.stack(
                [batch[key] for key in self.config.image_features], dim=-4
            )
        # Note: It's important that this happens after stacking the images into a single key.
        self._queues = populate_queues(self._queues, batch)

        if len(self._queues["action"]) == 0:
            # stack n latest observations from the queue
            batch = {k: torch.stack(
                list(self._queues[k]), dim=1) for k in batch if k in self._queues}
            actions = self.diffusion.generate_actions(batch)

            # TODO(rcadene): make above methods return output dictionary?
            actions = self.unnormalize_outputs({"action": actions})["action"]

            self._queues["action"].extend(actions.transpose(0, 1))

        action = self._queues["action"].popleft()
        return action

    def forward(self, batch: dict[str, Tensor]) -> tuple[Tensor, None]:
        """Run the batch through the model and compute the loss for training or validation."""
        batch = self.normalize_inputs(batch)
        if self.config.image_features:
            # shallow copy so that adding a key doesn't modify the original
            batch = dict(batch)
            batch["observation.images"] = torch.stack(
                [batch[key] for key in self.config.image_features], dim=-4
            )
        batch = self.normalize_targets(batch)
        loss = self.diffusion.compute_loss(batch)
        # no output_dict so returning None
        return loss, None


def _make_noise_scheduler(name: str, **kwargs: dict) -> DDPMScheduler | DDIMScheduler:
    """
    Factory for noise scheduler instances of the requested type. All kwargs are passed
    to the scheduler.
    """
    if name == "DDPM":
        return DDPMScheduler(**kwargs)
    elif name == "DDIM":
        return DDIMScheduler(**kwargs)
    else:
        raise ValueError(f"Unsupported noise scheduler type {name}")


class DiffusionModel(nn.Module):
    def __init__(self, config: DiffusionConfig):
        super().__init__()
        self.config = config

        # Build observation encoders (depending on which observations are provided).
        global_cond_dim = self.config.robot_state_feature.shape[0]
        if self.config.image_features:
            num_images = len(self.config.image_features)
            if self.config.use_separate_rgb_encoder_per_camera:
                encoders = [DiffusionRgbEncoder(config)
                            for _ in range(num_images)]
                self.rgb_encoder = nn.ModuleList(encoders)
                global_cond_dim += encoders[0].feature_dim * num_images
            else:
                self.rgb_encoder = DiffusionRgbEncoder(config)
                global_cond_dim += self.rgb_encoder.feature_dim * num_images
        if self.config.env_state_feature:
            global_cond_dim += self.config.env_state_feature.shape[0]

        self.unet = DiffusionConditionalUnet1d(
            config, global_cond_dim=global_cond_dim * config.n_obs_steps)

        self.noise_scheduler = _make_noise_scheduler(
            config.noise_scheduler_type,
            num_train_timesteps=config.num_train_timesteps,
            beta_start=config.beta_start,
            beta_end=config.beta_end,
            beta_schedule=config.beta_schedule,
            clip_sample=config.clip_sample,
            clip_sample_range=config.clip_sample_range,
            prediction_type=config.prediction_type,
        )

        if config.num_inference_steps is None:
            self.num_inference_steps = self.noise_scheduler.config.num_train_timesteps
        else:
            self.num_inference_steps = config.num_inference_steps

    # ========= inference  ============
    def conditional_sample(
        self, batch_size: int, global_cond: Tensor | None = None, generator: torch.Generator | None = None
    ) -> Tensor:
        device = get_device_from_parameters(self)
        dtype = get_dtype_from_parameters(self)

        # Sample prior.
        sample = torch.randn(
            size=(batch_size, self.config.horizon,
                  self.config.action_feature.shape[0]),
            dtype=dtype,
            device=device,
            generator=generator,
        )

        self.noise_scheduler.set_timesteps(self.num_inference_steps)

        for t in self.noise_scheduler.timesteps:
            # Predict model output.
            model_output = self.unet(
                sample,
                torch.full(sample.shape[:1], t,
                           dtype=torch.long, device=sample.device),
                global_cond=global_cond,
            )
            # Compute previous image: x_t -> x_t-1
            sample = self.noise_scheduler.step(
                model_output, t, sample, generator=generator).prev_sample

        return sample

    def _prepare_global_conditioning(self, batch: dict[str, Tensor]) -> Tensor:
        """Encode image features and concatenate them all together along with the state vector."""
        batch_size, n_obs_steps = batch[OBS_ROBOT].shape[:2]
        global_cond_feats = [batch[OBS_ROBOT]]
        # Extract image features.
        if self.config.image_features:
            if self.config.use_separate_rgb_encoder_per_camera:
                # Combine batch and sequence dims while rearranging to make the camera index dimension first.
                images_per_camera = einops.rearrange(
                    batch["observation.images"], "b s n ... -> n (b s) ...")
                img_features_list = torch.cat(
                    [
                        encoder(images)
                        for encoder, images in zip(self.rgb_encoder, images_per_camera, strict=True)
                    ]
                )
                # Separate batch and sequence dims back out. The camera index dim gets absorbed into the
                # feature dim (effectively concatenating the camera features).
                img_features = einops.rearrange(
                    img_features_list, "(n b s) ... -> b s (n ...)", b=batch_size, s=n_obs_steps
                )
            else:
                # Combine batch, sequence, and "which camera" dims before passing to shared encoder.
                img_features = self.rgb_encoder(
                    einops.rearrange(
                        batch["observation.images"], "b s n ... -> (b s n) ...")
                )
                # Separate batch dim and sequence dim back out. The camera index dim gets absorbed into the
                # feature dim (effectively concatenating the camera features).
                img_features = einops.rearrange(
                    img_features, "(b s n) ... -> b s (n ...)", b=batch_size, s=n_obs_steps
                )
            global_cond_feats.append(img_features)

        if self.config.env_state_feature:
            global_cond_feats.append(batch[OBS_ENV])

        # Concatenate features then flatten to (B, global_cond_dim).
        return torch.cat(global_cond_feats, dim=-1).flatten(start_dim=1)

    def generate_actions(self, batch: dict[str, Tensor]) -> Tensor:
        """
        This function expects `batch` to have:
        {
            "observation.state": (B, n_obs_steps, state_dim)

            "observation.images": (B, n_obs_steps, num_cameras, C, H, W)
                AND/OR
            "observation.environment_state": (B, environment_dim)
        }
        """
        batch_size, n_obs_steps = batch["observation.state"].shape[:2]
        assert n_obs_steps == self.config.n_obs_steps

        # Encode image features and concatenate them all together along with the state vector.
        global_cond = self._prepare_global_conditioning(
            batch)  # (B, global_cond_dim)

        # run sampling
        actions = self.conditional_sample(batch_size, global_cond=global_cond)

        # Extract `n_action_steps` steps worth of actions (from the current observation).
        start = n_obs_steps - 1
        end = start + self.config.n_action_steps
        actions = actions[:, start:end]

        return actions

    def compute_loss(self, batch: dict[str, Tensor]) -> Tensor:
        """
        This function expects `batch` to have (at least):
        {
            "observation.state": (B, n_obs_steps, state_dim)

            "observation.images": (B, n_obs_steps, num_cameras, C, H, W)
                AND/OR
            "observation.environment_state": (B, environment_dim)

            "action": (B, horizon, action_dim)
            "action_is_pad": (B, horizon)
        }
        """
        # Input validation.
        assert set(batch).issuperset(
            {"observation.state", "action", "action_is_pad"})
        assert "observation.images" in batch or "observation.environment_state" in batch
        n_obs_steps = batch["observation.state"].shape[1]
        horizon = batch["action"].shape[1]
        assert horizon == self.config.horizon
        assert n_obs_steps == self.config.n_obs_steps

        # Encode image features and concatenate them all together along with the state vector.
        global_cond = self._prepare_global_conditioning(
            batch)  # (B, global_cond_dim)

        # Forward diffusion.
        trajectory = batch["action"]
        # Sample noise to add to the trajectory.
        eps = torch.randn(trajectory.shape, device=trajectory.device)
        # Sample a random noising timestep for each item in the batch.
        timesteps = torch.randint(
            low=0,
            high=self.noise_scheduler.config.num_train_timesteps,
            size=(trajectory.shape[0],),
            device=trajectory.device,
        ).long()
        # Add noise to the clean trajectories according to the noise magnitude at each timestep.
        noisy_trajectory = self.noise_scheduler.add_noise(
            trajectory, eps, timesteps)

        # Run the denoising network (that might denoise the trajectory, or attempt to predict the noise).
        pred = self.unet(noisy_trajectory, timesteps, global_cond=global_cond)

        # Compute the loss.
        # The target is either the original trajectory, or the noise.
        if self.config.prediction_type == "epsilon":
            target = eps
        elif self.config.prediction_type == "sample":
            target = batch["action"]
        else:
            raise ValueError(
                f"Unsupported prediction type {self.config.prediction_type}")

        loss = F.mse_loss(pred, target, reduction="none")

        # Mask loss wherever the action is padded with copies (edges of the dataset trajectory).
        if self.config.do_mask_loss_for_padding:
            if "action_is_pad" not in batch:
                raise ValueError(
                    "You need to provide 'action_is_pad' in the batch when "
                    f"{self.config.do_mask_loss_for_padding=}."
                )
            in_episode_bound = ~batch["action_is_pad"]
            loss = loss * in_episode_bound.unsqueeze(-1)

        return loss.mean()


class SinusoidalPosEmb(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim

    def forward(self, x):
        device = x.device
        half_dim = self.dim // 2
        emb = math.log(10000) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)
        emb = x[:, None] * emb[None, :]
        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)
        return emb


class AdaLN(nn.Module):
    def __init__(self, num_features, num_conds):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(num_features))
        self.beta = nn.Parameter(torch.zeros(num_features))
        self.cond_to_gamma = nn.Linear(num_conds, num_features)
        self.cond_to_beta = nn.Linear(num_conds, num_features)

    def forward(self, x, z):
        gamma = self.cond_to_gamma(z) + self.gamma
        beta = self.cond_to_beta(z) + self.beta
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True, unbiased=False)
        x_norm = (x - mean) / (std + 1e-5)
        return gamma * x_norm + beta


class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, num_conds):
        super().__init__()
        self.norm = AdaLN(in_channels, num_conds)
        self.fc1 = nn.Linear(in_channels, out_channels)
        self.act = nn.SiLU()
        self.fc2 = nn.Linear(out_channels, out_channels)
        self.res = (in_channels == out_channels)

    def forward(self, x, z):
        res = x
        out = self.norm(x, z)
        out = self.act(self.fc1(out))
        out = self.fc2(out)
        if self.res:
            return out + res
        else:
            raise ValueError("Residual dims do not match.")


class DenoisingMLP(nn.Module):
    def __init__(self, in_channels, out_channels, num_blocks=3, width=1024, num_conds=None):
        super().__init__()
        assert num_conds is not None, "num_conds must be set for AdaLN"
        layers = []
        dims = [in_channels] + [width] * (num_blocks - 1)
        self.blocks = nn.ModuleList([
            ResidualBlock(dims[i], dims[i+1], num_conds)
            for i in range(len(dims)-1)
        ])
        self.final = nn.Linear(
            width, out_channels) if out_channels != width else nn.Identity()

    def forward(self, x, z, t):
        # x: [batch2, in_channels], z: [batch2, num_conds], t: [batch2, num_conds]
        cond = z + t
        out = x
        for blk in self.blocks:
            out = blk(out, cond)
        return self.final(out)


class DiffusionModel(nn.Module):
    """
    MLP-based diffusion denoiser with time embeddings.
    """

    def __init__(self, state_dim: int, hidden_dim: int = 128, num_layers: int = 3):
        super().__init__()
        self.time_emb = SinusoidalPosEmb(hidden_dim)
        self.denoiser = DenoisingMLP(
            in_channels=state_dim,
            out_channels=state_dim,
            num_blocks=num_layers,
            width=hidden_dim,
            num_conds=hidden_dim,
        )

    def forward(self, x: torch.Tensor, timesteps: torch.Tensor, goal: torch.Tensor = None) -> torch.Tensor:
        # x: [B, T, D]
        B, T, D = x.shape
        x_flat = x.view(B * T, D)
        t_emb = self.time_emb(timesteps).repeat_interleave(T, dim=0)
        # use goal as zero if not provided
        if goal is None:
            cond = torch.zeros_like(t_emb)
        else:
            cond = goal.unsqueeze(1).repeat(1, T, 1).view(B * T, -1)
        out_flat = self.denoiser(x_flat, cond, t_emb)
        return out_flat.view(B, T, D)


# Optional: from .transformer_core import LLaMACore # If using a custom core

# Placeholder for Sinusoidal Position Embedding


class SinusoidalPosEmb(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim

    def forward(self, x):
        device = x.device
        half_dim = self.dim // 2
        import math
        emb = math.log(10000) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)
        # Input x is expected to be timesteps [B,]
        emb = x[:, None] * emb[None, :]
        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)
        return emb


class VisionConditionedDiffusionPolicy(nn.Module):
    def __init__(self,
                 image_tokenizer: ImageTokenizer,
                 denoising_head: DenoisingHead,
                 scheduler_wrapper: DiffusionSchedulerWrapper,
                 action_dim: int,
                 transformer_dim: int = 512,
                 num_transformer_layers: int = 6,
                 transformer_heads: int = 8,
                 task_embed_dim: Optional[int] = None,
                 use_task_embedding: bool = False,
                 prediction_type: str = 'epsilon',  # or 'sample'
                 ):
        super().__init__()
        self.image_tokenizer = image_tokenizer
        self.denoising_head = denoising_head
        self.scheduler = scheduler_wrapper
        self.action_dim = action_dim
        self.transformer_dim = transformer_dim
        self.use_task_embedding = use_task_embedding
        self.prediction_type = prediction_type

        # --- Embeddings ---
        self.time_embed = SinusoidalPosEmb(transformer_dim)
        self.action_embed = nn.Linear(action_dim, transformer_dim)
        if use_task_embedding:
            assert task_embed_dim is not None, "task_embed_dim must be provided if use_task_embedding is True"
            # Simple projection, could be more complex (e.g., another transformer)
            self.task_embed = nn.Linear(task_embed_dim, transformer_dim)
        else:
            self.task_embed = None

        # --- Transformer Decoder ---
        # Using standard PyTorch TransformerDecoder. Replace with LLaMACore if needed.
        decoder_layer = nn.TransformerDecoderLayer(
            d_model=transformer_dim,
            nhead=transformer_heads,
            dim_feedforward=transformer_dim * 4,  # Common practice
            batch_first=True
        )
        self.transformer_decoder = nn.TransformerDecoder(
            decoder_layer, num_layers=num_transformer_layers)

        # --- Positional Encoding for Actions ---
        # Learnable positional embedding for the action sequence length
        # Max sequence length needs to be defined (e.g., action horizon)
        # Placeholder: Assume max action sequence length T_a
        # self.action_pos_embed = nn.Parameter(torch.zeros(1, MAX_ACTION_SEQ_LEN, transformer_dim))

    def get_condition_tokens(self, images: torch.Tensor, task_embedding: Optional[torch.Tensor] = None) -> torch.Tensor:
        """Combine vision and optional task embeddings into context tokens."""
        vision_tokens = self.image_tokenizer(
            images)  # [B, N_vision, D_transformer]

        if self.use_task_embedding and task_embedding is not None:
            # Process task embedding
            # If task_embedding is [B, D_task], project and unsqueeze
            if task_embedding.ndim == 2:
                task_tokens = self.task_embed(task_embedding).unsqueeze(
                    1)  # [B, 1, D_transformer]
            # If task_embedding is [B, T_task, D_task], project each step
            elif task_embedding.ndim == 3:
                # [B, T_task, D_transformer]
                task_tokens = self.task_embed(task_embedding)
            else:
                raise ValueError("Invalid task embedding shape")
            # Concatenate vision and task tokens
            # [B, N_vision + N_task, D_transformer]
            context_tokens = torch.cat([vision_tokens, task_tokens], dim=1)
        else:
            context_tokens = vision_tokens  # [B, N_vision, D_transformer]

        return context_tokens

    def forward(self,
                images: torch.Tensor,
                actions: torch.Tensor,
                timesteps: torch.Tensor,
                task_embedding: Optional[torch.Tensor] = None,
                cond_mask: Optional[torch.Tensor] = None  # For CFG
                ) -> torch.Tensor:
        """
        Training forward pass.
        images: [B, T_img, C, H, W]
        actions: [B, T_act, A] (clean actions)
        timesteps: [B,]
        task_embedding: Optional [B, D_task] or [B, T_task, D_task]
        cond_mask: Optional [B,] mask for dropping conditioning (CFG)
        Returns: Predicted noise (epsilon) or predicted clean sample (x0) [B, T_act, A]
        """
        B, T_act, A = actions.shape
        device = actions.device

        # 1. Add noise to actions
        noisy_actions, noise = self.scheduler.add_noise(actions, timesteps)

        # 2. Get condition tokens (vision + task)
        context_tokens = self.get_condition_tokens(images, task_embedding)

        # --- Classifier-Free Guidance (CFG) during training ---
        if cond_mask is not None and self.training:
            # Create unconditional context (e.g., zero out vision/task tokens or use learned null embeddings)
            # Simple approach: zero out context for masked samples
            # A better approach might involve dedicated null embeddings
            uncond_context_tokens = torch.zeros_like(context_tokens)
            context_tokens = torch.where(cond_mask.view(B, 1, 1).expand_as(context_tokens),
                                         context_tokens,
                                         uncond_context_tokens)

        # 3. Embed inputs
        time_emb = self.time_embed(timesteps).unsqueeze(
            1)  # [B, 1, D_transformer]
        action_emb = self.action_embed(
            noisy_actions)  # [B, T_act, D_transformer]

        # Add positional encoding to actions if defined
        # if hasattr(self, 'action_pos_embed'):
        #     action_emb = action_emb + self.action_pos_embed[:, :T_act, :]

        # 4. Prepare Transformer inputs
        # Target sequence for decoder is noisy actions + time embedding
        # Memory for decoder is the context tokens
        # Prepend time embedding to the action sequence
        # [B, 1 + T_act, D_transformer]
        decoder_input = torch.cat([time_emb, action_emb], dim=1)

        # Create target mask (causal mask for decoder input)
        tgt_mask = nn.Transformer.generate_square_subsequent_mask(
            decoder_input.size(1), device=device)

        # 5. Run Transformer Decoder
        transformer_output = self.transformer_decoder(
            tgt=decoder_input,
            memory=context_tokens,
            tgt_mask=tgt_mask
            # Optional: memory_mask, tgt_key_padding_mask, memory_key_padding_mask
        )
        # Output is [B, 1 + T_act, D_transformer], take only action sequence part
        # [B, T_act, D_transformer]
        action_output_tokens = transformer_output[:, 1:, :]

        # 6. Denoising Head
        predicted_output = self.denoising_head(
            action_output_tokens)  # [B, T_act, A]

        # 7. Return based on prediction type
        if self.prediction_type == 'epsilon':
            return predicted_output  # Model predicts noise
        elif self.prediction_type == 'sample':
            return predicted_output  # Model predicts clean sample x0
        else:
            raise ValueError(
                f"Unknown prediction type: {self.prediction_type}")

    @torch.no_grad()
    def inference(self,
                  images: torch.Tensor,
                  num_inference_steps: int,
                  task_embedding: Optional[torch.Tensor] = None,
                  guidance_scale: float = 1.0,  # For CFG
                  # If different from training
                  action_horizon: Optional[int] = None,
                  initial_noise: Optional[torch.Tensor] = None
                  ) -> torch.Tensor:
        """
        Autoregressive inference loop.
        images: [B, T_img, C, H, W]
        num_inference_steps: Number of DDPM/DDIM steps
        task_embedding: Optional [B, D_task] or [B, T_task, D_task]
        guidance_scale: Strength of CFG. 1.0 means no guidance.
        action_horizon: Length of the action sequence to generate.
        initial_noise: Optional initial noise tensor [B, T_act, A]
        Returns: Denoised action sequence [B, T_act, A]
        """
        B = images.shape[0]
        device = images.device
        T_act = action_horizon if action_horizon is not None else self.scheduler.config.get(
            'train_timesteps', 1000)  # Placeholder, adjust as needed

        # 1. Prepare scheduler and initial noise
        self.scheduler.set_timesteps(num_inference_steps, device=device)
        if initial_noise is None:
            noisy_actions = torch.randn(
                (B, T_act, self.action_dim), device=device)
        else:
            assert initial_noise.shape == (B, T_act, self.action_dim)
            noisy_actions = initial_noise

        # 2. Get condition tokens
        cond_tokens = self.get_condition_tokens(images, task_embedding)

        # Prepare unconditional tokens if using CFG
        uncond_tokens = None
        if guidance_scale > 1.0:
            # Simple approach: zero out context
            # Better: use dedicated null embeddings learned during training
            uncond_tokens = torch.zeros_like(cond_tokens)

        # 3. Denoising loop
        for t in self.scheduler.timesteps:
            # Expand timestep for batch
            timesteps_batch = torch.tensor(
                [t] * B, device=device, dtype=torch.long)

            # --- CFG Prediction ---
            if guidance_scale > 1.0 and uncond_tokens is not None:
                # Predict noise for both conditional and unconditional inputs
                # Combine inputs for a single forward pass
                combined_context = torch.cat(
                    [cond_tokens, uncond_tokens], dim=0)
                combined_noisy_actions = torch.cat(
                    [noisy_actions, noisy_actions], dim=0)
                combined_timesteps = torch.cat(
                    [timesteps_batch, timesteps_batch], dim=0)

                # Embed inputs for combined batch
                time_emb = self.time_embed(combined_timesteps).unsqueeze(1)
                action_emb = self.action_embed(combined_noisy_actions)
                # if hasattr(self, 'action_pos_embed'): action_emb += self.action_pos_embed[:, :T_act, :]
                decoder_input = torch.cat([time_emb, action_emb], dim=1)
                tgt_mask = nn.Transformer.generate_square_subsequent_mask(
                    decoder_input.size(1), device=device)

                # Run transformer
                transformer_output = self.transformer_decoder(
                    decoder_input, combined_context, tgt_mask=tgt_mask)
                action_output_tokens = transformer_output[:, 1:, :]

                # Run denoising head
                model_output_combined = self.denoising_head(
                    action_output_tokens)

                # Split predictions
                model_output_cond, model_output_uncond = torch.chunk(
                    model_output_combined, 2, dim=0)

                # Combine predictions using guidance scale
                model_output = model_output_uncond + guidance_scale * \
                    (model_output_cond - model_output_uncond)
            else:
                # Standard prediction without CFG
                time_emb = self.time_embed(timesteps_batch).unsqueeze(1)
                action_emb = self.action_embed(noisy_actions)
                # if hasattr(self, 'action_pos_embed'): action_emb += self.action_pos_embed[:, :T_act, :]
                decoder_input = torch.cat([time_emb, action_emb], dim=1)
                tgt_mask = nn.Transformer.generate_square_subsequent_mask(
                    decoder_input.size(1), device=device)

                transformer_output = self.transformer_decoder(
                    decoder_input, cond_tokens, tgt_mask=tgt_mask)
                action_output_tokens = transformer_output[:, 1:, :]
                model_output = self.denoising_head(action_output_tokens)

            # 4. Scheduler step
            noisy_actions = self.scheduler.step(model_output, t, noisy_actions)

        # Return the final denoised actions
        return noisy_actions


class DenoisingHead(nn.Module):
    """Simple MLP head to predict the denoised actions."""

    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Input: x [B, T, D_transformer_output]
        Output: [B, T, D_action]
        """
        return self.net(x)
